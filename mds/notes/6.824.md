# [Lecture 1](https://www.youtube.com/watch?v=cQP8WApzIQQ&list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB&index=1&t=3s)

> **distributed systems** is a set of cooperating computers that communicate with eachother over a network to get some task done

if it's possible to solve a problem with 1 computer, without building a distributed system, then do that way

why use/build a distributed system?

- parallelism
- fault tolerance
- physical reasons
- security/isolation

the course will focus on the first 2 points

why ds is hard?

- concurrency
- network interations (partial failure)
- performance

the infrastructure/systems course coverage is: 

- **storage**
- computation
- communication

the critical tools to build a ds are:

- RPC (to mask/abstract the network)
- threads
- concurrency locks etc...

## Performance things

- <ins>Scalability</ins>\
**|** 2x computers -> 2x performance/throughput\
**|** huge win business side, you just buy more computers and don't pay programmers to optimize the software
- <ins>Fault tolerance</ins>\
**|** 1000 computers =~ 3 computer failures per day\
**|** the 'behaviour' of masking a failure needs to be built in the design\
**|** implemented with replicas
- <ins>Availability</ins>\
**|** the system is operating/available despite the failure
- <ins>Recoverability</ins>\
**|** recover from a failure without any loss of correctness\
**|** save the latest state in a Non-Volatile storage so it could be used to restore after the failure, but it's expensive
- <ins>Consistency</ins>\
**|** imagine key-value services: Put(k,v) and Get(k) -> v, a definition of consistency could be: 'a Get yelds thevalue put by the most recent completed Put' (**Strong consistency**, expensive)\
**|** **Weak consistency** is often used

## MapReduce

designed, built, used by Google

### The idea

they were running huge computations on TB and TB of datas, specifically for indexes and doing indexing is basically running a sort algorithm

so they just said, ok let's run this computations split on thousands of computers

first they needed specialized engineers to write these distributed systems software in details for every operation they needed to make, after a while they said: "ok let's build a framework to make it easy for non-specialist to write these softwares"

### The flow

**master server** that send inputs files to **workers**

we have inputs split across different files

all operations are done in **parallel**

~~~js
files inputs ->
indipendent Map functions that produces a list of kv pairs -> 
collect all the same keys ->
send the kv pairs to a different Reduce function -> 
results of Reduce functions are the outputs of MapReduce
~~~

![mapreduce-diagram-img](https://imgur.com/LTWXUve.jpg)

of course there could be multiple mapreduces in sequence

a core component in Google when operating with mapreduce is the **GFS** (google file system) that is really cool and do a lot of things

### high level implementation example of mapreduce (word count)

![mapreduce-wordcount-img](https://imgur.com/BpPdCHQ.jpg)

**[lab1](https://pdos.csail.mit.edu/6.824/labs/lab-mr.html)**

**[mapreduce paper](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)**

